{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Package Imports \n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gisette Dataset Pre-Processing\n",
    "\n",
    "def preprocessing_y(filename):\n",
    "    data = []\n",
    "    with open(filename) as file:\n",
    "        for row in file.readlines():\n",
    "            data.append((row.strip()).split(\" \"))\n",
    "    data = np.array(data).astype(int)\n",
    "    data = np.squeeze(data, axis=1)\n",
    "    return data\n",
    "\n",
    "def preprocessing_x(filename):\n",
    "    data = []\n",
    "    with open(filename) as file:\n",
    "        for row in file.readlines():\n",
    "            data.append((row.strip()).split(\" \"))\n",
    "    data = np.array(data).astype(int)\n",
    "    return data\n",
    "\n",
    "# Here the shape of x_train is (5000, 6000) = (dimension, number of examples)\n",
    "\n",
    "x_train = preprocessing_x(\"gisette_train.data\")\n",
    "y_train = preprocessing_y(\"gisette_train.labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we define the functions used in the optimization part \n",
    "    \n",
    "def loss(theta, lbda):\n",
    "    bAx = y_train * np.dot(x_train, theta)\n",
    "    return np.mean(np.log(1. + np.exp(- bAx))) + lbda * np.linalg.norm(theta) ** 2 / 2.\n",
    "\n",
    "def grad(theta, lbda, n):\n",
    "    bAx = y_train * np.dot(x_train, theta)\n",
    "    temp = 1. / (1. + np.exp(bAx))\n",
    "    grad = - np.dot(x_train.T, y_train * temp) / n + lbda * theta\n",
    "    return grad\n",
    "\n",
    "def grad_i(i, theta, lbda):\n",
    "    grad = - x_train[i] * y_train[i] / (1. + np.exp(y_train[i] * np.dot(x_train[i], theta)))\n",
    "    grad += lbda * theta\n",
    "    return grad\n",
    "\n",
    "def lipschitz_constant(lbda):\n",
    "    return np.linalg.norm(x_train, ord=2) ** 2 / (4. * n) + lbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After iteration 0 loss is now:  1.2370983846568533\n",
      "After iteration 1 loss is now:  0.4155042710489129\n",
      "After iteration 2 loss is now:  0.3183698314391979\n",
      "After iteration 3 loss is now:  0.2927373692944291\n",
      "After iteration 4 loss is now:  0.2581287386028342\n",
      "After iteration 5 loss is now:  0.24131418588766826\n",
      "After iteration 6 loss is now:  0.22700572674636588\n",
      "After iteration 7 loss is now:  0.21574836593558397\n",
      "After iteration 8 loss is now:  0.20665044863677662\n",
      "After iteration 9 loss is now:  0.19907707020245716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.59454013e-06, -5.60258412e-06, -2.54240195e-05, ...,\n",
       "       -9.19156035e-06,  4.68052528e-07, -3.14511049e-05])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SVRG\n",
    "\n",
    "def optimize_svrg(itr1, itr2, x_train, y_train):\n",
    "    n = x_train.shape[0]\n",
    "    d = x_train.shape[1]\n",
    "    lbda = 1. / n ** (0.5)\n",
    "    stepsize = 1 / lipschitz_constant(lbda)\n",
    "    theta_init = np.zeros(d)\n",
    "    theta_bar = theta_init.copy()\n",
    "    for k in range(itr1):\n",
    "        grad_k =  grad(theta_bar, lbda, n)\n",
    "        theta = theta_bar.copy()\n",
    "        for t in range(itr2):\n",
    "            i = np.random.randint(1, n)\n",
    "            theta = theta - stepsize * (grad_i(i, theta, lbda) - grad_i(i, theta_bar, lbda) + grad_k)    \n",
    "        theta_bar = theta\n",
    "        print(\"After iteration {} loss is now: \".format(k), loss(theta_bar, lbda))\n",
    "    return theta_bar\n",
    "\n",
    "optimize_svrg(10, 10, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SVRG2\n",
    "\n",
    "def svrg2(itr1,itr2, stepsize, dim):\n",
    "    return None\n",
    "\n",
    "## CURVATURE MATCHING\n",
    "\n",
    "def curvature_matching(f, stepsize):\n",
    "    return None\n",
    "\n",
    "## ACTION MATCHING\n",
    "\n",
    "def action_matching(f, stepsize):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
